import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
from nltk.util import ngrams
import string
import pandas as pd

# Download NLTK resources if not already downloaded
#nltk.download('punkt')
#nltk.download('stopwords')


# List of comments
#comments = ["I really enjoyed the product, it's great!", "The service was terrible, very disappointed.","The packaging was good, but the product itself was lacking.", "Overall, I'm satisfied with my purchase."]

comments_xl = pd.read_excel("/workspaces/codespaces-jupyter/test_comments.xlsx")
comments = comments_xl["Public Comment"].tolist()

###PULSE CHECK - LAUREN####


###TOPICS - RISHABH###
#Accessibility: Biking, walking, pedestrian, parking, car, transit, bus, ferry, train, NJ TRANSIT, light rail, shuttle
#Amenities: Bike share, garden, excercise, playground, play area, pool, courts, swim, fields, board, kayak, play, sports
#Ecology/Resilience: Wildlife, nature, contamination, chromium, cleanup, erosion, ecosystem, water, climate, wetland, habitat, bird, insect.
#Concepts: People's Park, Interior Habitats, afterthought
#Type of development: privatisation, commercialization, community park, festivals, concerts, stadium, arena, sports venue, amphitheater, community, Caven Point

#for loop: if comment contains accessibilty, then accessibility += 1, then compare counts

##maybe do the topics here?

# Function to preprocess text
def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove punctuation
    tokens = [token for token in tokens if token not in string.punctuation]
    # Convert tokens to lowercase
    tokens = [token.lower() for token in tokens]
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    return tokens

# Preprocess all comments and combine into a single list of tokens
all_tokens = [preprocess_text(comment) for comment in comments]
all_tokens_flat = [token for sublist in all_tokens for token in sublist]



# Count occurrences of each token
word_freq = Counter(all_tokens_flat)

# Get the most common words and their frequencies
top_n = 10
top_words = word_freq.most_common(top_n)

# Display the top words
print("Top", top_n, "most common words:")
for word, freq in top_words:
    print(word, "-", freq)

#ADD ADDITIONAL PHRASE LENGTHS
# Extract and count n-grams (phrases)
n = 2  # Change n to extract different n-grams, e.g., bigrams (n=2) or trigrams (n=3)
phrases = []
for comment_tokens in all_tokens:
    comment_phrases = list(ngrams(comment_tokens, n))
    phrases.extend(comment_phrases)

# Count occurrences of each phrase
phrase_freq = Counter(phrases)

# Get the most common phrases and their frequencies
top_n_phrases = 10
top_phrases = phrase_freq.most_common(top_n_phrases)

# Display the top phrases
print("\nTop", top_n_phrases, "most common phrases ({}-grams):".format(n))
for phrase, freq in top_phrases:
    print(' '.join(phrase), "-", freq)