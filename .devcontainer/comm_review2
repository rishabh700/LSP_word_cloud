import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
from nltk.util import ngrams
import string
import pandas as pd
from textblob import TextBlob
from openpyxl import load_workbook

# List of comments

publiccomms_1 = pd.read_csv("/workspaces/codespaces-jupyter/comments_all/liberty-state-park-design-task-force-2024-03-12.csv")
publiccomms_2 = pd.read_csv("/workspaces/codespaces-jupyter/comments_all/LSP - Design Task Force - Public Comments - March 2 to March 16.csv")
publiccomms_3 = pd.read_csv("/workspaces/codespaces-jupyter/comments_all/LSP 875 Public Comments.csv")

comments_xl = publiccomms_1.append(publiccomms_2, ignore_index=True)
comments_xl['Name'] = comments_xl['Name (First)'] + ' ' + comments_xl['Name (Last)']
comments_xl = comments_xl[['Name', 'Email Address', 'Public Comment']]

publiccomms_3.rename(columns={'Comment': 'Public Comment'}, inplace=True)

comments_xl = publiccomms_3.append(comments_xl, ignore_index=True)

###PULSE CHECK - LAUREN####

positivewords = ['I support', 'in favor of', "I like"]
negativewords = ['against', "oppose", "reject", "not agree", "bad", "afterthought", "doesn't cut it"]

# Function to check if any word from the list is in the text
def check_word(text):
    for word in positivewords:
        if word in text:
            return "supportive"
    for word in negativewords:
        if word in text:
            return "unsupportive"
    return "neutral"

comments_xl['emotion'] = comments_xl['Public Comment'].apply(check_word)
summary = comments_xl.groupby('emotion')['Public Comment'].count()
print(summary)

###TOPICS - RISHABH###
#Accessibility: Biking, walking, pedestrian, parking, car, transit, bus, ferry, train, NJ TRANSIT, light rail, shuttle
#Amenities: Playground, play area, pool, courts, swim, fields, boad, kayak, play, sports
#Ecology/Resilience: Wildlife, nature, contamination, chromium, cleanup, erosion, ecosystem, water, climate, wetland
#Concepts: People's Park, Interior Habitats, afterthought
#Type of development: privatisation, commercialization, community park, festivals, concerts, stadium, arena, sports venue, amphitheater

#for loop: if comment contains accessibilty, then accessibility += 1, then compare counts

##maybe do the topics here?




#COUNT WORDS

comments = comments_xl["Public Comment"].tolist()

# Function to preprocess text
def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove punctuation
    tokens = [token for token in tokens if token not in string.punctuation]
    # Convert tokens to lowercase
    tokens = [token.lower() for token in tokens]
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    return tokens

# Preprocess all comments and combine into a single list of tokens
all_tokens = [preprocess_text(comment) for comment in comments]
all_tokens_flat = [token for sublist in all_tokens for token in sublist]



# Count occurrences of each token
word_freq = Counter(all_tokens_flat)

# Get the most common words and their frequencies
top_n = 10
top_words = word_freq.most_common(top_n)

# Display the top words
print("Top", top_n, "most common words:")
for word, freq in top_words:
    print(word, "-", freq)

#ADD ADDITIONAL PHRASE LENGTHS
# Extract and count n-grams (phrases)
n = 2  # Change n to extract different n-grams, e.g., bigrams (n=2) or trigrams (n=3)
phrases = []
for comment_tokens in all_tokens:
    comment_phrases = list(ngrams(comment_tokens, n))
    phrases.extend(comment_phrases)

# Count occurrences of each phrase
phrase_freq = Counter(phrases)

# Get the most common phrases and their frequencies
top_n_phrases = 10
top_phrases = phrase_freq.most_common(top_n_phrases)

# Display the top phrases
print("\nTop", top_n_phrases, "most common phrases ({}-grams):".format(n))
for phrase, freq in top_phrases:
    print(' '.join(phrase), "-", freq)